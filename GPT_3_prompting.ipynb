{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1QoBNjvbtQoE7R13JqwKqrVChTIkBDzl1",
      "authorship_tag": "ABX9TyNn7PRd1HPQpVOB1K+rTDbl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SherinBojappa/prompting/blob/main/GPT_3_prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "KImd9fVGE9iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "CsYXAAIRFDs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open ai api key\n",
        "%cd /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "dSQOko6EFRp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas "
      ],
      "metadata": {
        "id": "qcqELxCdJj5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "FxVQcW6-JoRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get openai api key from dashboard\n",
        "f = open('open_ai_key.txt', 'r')\n",
        "key = f.readlines()\n",
        "f.close()\n",
        "openai.api_key = key[0].strip('\\n')"
      ],
      "metadata": {
        "id": "nBHJhGW-lB4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zero shot performance does not give the required label\n",
        "p = \"\"\"premise: 'Britain said, Friday, that it has barred cleric, Omar Bakri, from returning to the country from Lebanon, \n",
        "where he was released by police after being detained for 24 hours.\n",
        "hypothesis: 'Bakri was briefly detained, but was released.\n",
        "relation: \"\"\""
      ],
      "metadata": {
        "id": "yta2V_ImDOzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Completion.create(engine='text-davinci-001', prompt=p, max_tokens = 60)"
      ],
      "metadata": {
        "id": "7YQumuWdDZ4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zR98gqbDdii",
        "outputId": "fe1f42b9-a6f7-4dbf-ec46-5e8f89a69f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \"\\n\\nOmar Bakri was barred from returning to Britain from Lebanon.\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1666716652,\n",
            "  \"id\": \"cmpl-65HQa9f1rN7gom46LXuMdIQVSeDJq\",\n",
            "  \"model\": \"text-davinci-001\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 15,\n",
            "    \"prompt_tokens\": 63,\n",
            "    \"total_tokens\": 78\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# query gpt with a prompt\n",
        "prompt = \"\"\" Decide whether the sentence's sentiment is positive or negative\n",
        "sentence: I hate this movie!\n",
        "sentiment:\"\"\""
      ],
      "metadata": {
        "id": "rXC-53mu-S_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query gpt with a prompt\n",
        "prompt = \"\"\" Decide whether the relation between premise and hypothesis have entailment or contradiction\n",
        "premise: No Weapons of Mass Destruction Found in Iraq Yet.\n",
        "hypothesis: Weapons of Mass Destruction Found in Iraq.\n",
        "relation: not entailment\n",
        "\n",
        "premise: 'Britain said, Friday, that it has barred cleric, Omar Bakri, from returning to the country from Lebanon, \n",
        "where he was released by police after being detained for 24 hours.\n",
        "hypothesis: 'Bakri was briefly detained, but was released.\n",
        "relation: \n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MFajgGWoQMcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Completion.create(engine='text-davinci-001', prompt=prompt, max_tokens = 60)\n",
        "#response = openai.Completion.create(engine='text-ada-001', prompt=prompt, max_tokens = 6)"
      ],
      "metadata": {
        "id": "zaSy-qbHLYWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10hz2FMlLwOp",
        "outputId": "a18da3c2-fa42-4066-d1c9-7be899c6dbf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \"\\nnot entailment\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1666716077,\n",
            "  \"id\": \"cmpl-65HHJgpmyghC6mc6SHe3xKbBJ0nyu\",\n",
            "  \"model\": \"text-davinci-001\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 4,\n",
            "    \"prompt_tokens\": 113,\n",
            "    \"total_tokens\": 117\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response[\"choices\"][0][\"text\"].strip('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YyRJAdeLf0BP",
        "outputId": "abd51de3-f537-4c1c-9e33-fd3e2dad25da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'not entailment'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get rte dataset from huggingface\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "t-OK-zw6MVsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets as datasets"
      ],
      "metadata": {
        "id": "FIaxCCLkMd3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RTE dataset from Super GLUE - https://paperswithcode.com/sota/natural-language-inference-on-rte\n",
        "# 0 - enatilment\n",
        "# 1 - not entailment\n",
        "dataset = 'rte'"
      ],
      "metadata": {
        "id": "c5vk2sEaJZfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset == 'rte':\n",
        "  train_set = datasets.load_dataset('super_glue',dataset, split='train')\n",
        "  dev_set = datasets.load_dataset('super_glue', dataset, split='validation')"
      ],
      "metadata": {
        "id": "xu9jpnC7M76B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8Zq_tvYLlK4",
        "outputId": "3bc02b7f-a702-4588-c7c7-13b0c028c869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2490, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_set.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-h1aYaVCq5S",
        "outputId": "6c89e900-4a01-4257-ac64-2c49af0914b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(277, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC-yX5BNOiJl",
        "outputId": "993e41c6-67ea-4e80-f6ea-fd0de405ded9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'premise': 'Britain said, Friday, that it has barred cleric, Omar Bakri, from returning to the country from Lebanon, where he was released by police after being detained for 24 hours.',\n",
              " 'hypothesis': 'Bakri was briefly detained, but was released.',\n",
              " 'idx': 5,\n",
              " 'label': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new toy dataset with very few examples from train dataset\n",
        "new_toy_set = {}"
      ],
      "metadata": {
        "id": "LnNl89HLdQf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,5):\n",
        "  new_toy_set[i] = train_set[i]"
      ],
      "metadata": {
        "id": "pjw9hDS8dSAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_toy_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XcbT1goeP8o",
        "outputId": "2537019f-1cf7-4f59-c4b3-705557284d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {'premise': 'No Weapons of Mass Destruction Found in Iraq Yet.',\n",
              "  'hypothesis': 'Weapons of Mass Destruction Found in Iraq.',\n",
              "  'idx': 0,\n",
              "  'label': 1},\n",
              " 1: {'premise': 'A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.',\n",
              "  'hypothesis': 'Pope Benedict XVI is the new leader of the Roman Catholic Church.',\n",
              "  'idx': 1,\n",
              "  'label': 0},\n",
              " 2: {'premise': 'Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients.',\n",
              "  'hypothesis': 'Herceptin can be used to treat breast cancer.',\n",
              "  'idx': 2,\n",
              "  'label': 0},\n",
              " 3: {'premise': 'Judie Vivian, chief executive at ProMedica, a medical service company that helps sustain the 2-year-old Vietnam Heart Institute in Ho Chi Minh City (formerly Saigon), said that so far about 1,500 children have received treatment.',\n",
              "  'hypothesis': 'The previous name of Ho Chi Minh City was Saigon.',\n",
              "  'idx': 3,\n",
              "  'label': 0},\n",
              " 4: {'premise': \"A man is due in court later charged with the murder 26 years ago of a teenager whose case was the first to be featured on BBC One's Crimewatch. Colette Aram, 16, was walking to her boyfriend's house in Keyworth, Nottinghamshire, on 30 October 1983 when she disappeared. Her body was later found in a field close to her home. Paul Stewart Hutchinson, 50, has been charged with murder and is due before Nottingham magistrates later.\",\n",
              "  'hypothesis': 'Paul Stewart Hutchinson is accused of having stabbed a girl.',\n",
              "  'idx': 4,\n",
              "  'label': 1}}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,5):\n",
        "  new_toy_set[i]['template'] = ' Given hypothesis is this the right prediction? '"
      ],
      "metadata": {
        "id": "8VebIwJ7eRwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_toy_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UltJpF_Dep-Q",
        "outputId": "0d7d1ff9-6bd2-4146-9887-39f528b35a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {'premise': 'No Weapons of Mass Destruction Found in Iraq Yet.',\n",
              "  'hypothesis': 'Weapons of Mass Destruction Found in Iraq.',\n",
              "  'idx': 0,\n",
              "  'label': 1,\n",
              "  'template': ' Given hypothesis is this the right prediction? '},\n",
              " 1: {'premise': 'A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.',\n",
              "  'hypothesis': 'Pope Benedict XVI is the new leader of the Roman Catholic Church.',\n",
              "  'idx': 1,\n",
              "  'label': 0,\n",
              "  'template': ' Given hypothesis is this the right prediction? '},\n",
              " 2: {'premise': 'Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients.',\n",
              "  'hypothesis': 'Herceptin can be used to treat breast cancer.',\n",
              "  'idx': 2,\n",
              "  'label': 0,\n",
              "  'template': ' Given hypothesis is this the right prediction? '},\n",
              " 3: {'premise': 'Judie Vivian, chief executive at ProMedica, a medical service company that helps sustain the 2-year-old Vietnam Heart Institute in Ho Chi Minh City (formerly Saigon), said that so far about 1,500 children have received treatment.',\n",
              "  'hypothesis': 'The previous name of Ho Chi Minh City was Saigon.',\n",
              "  'idx': 3,\n",
              "  'label': 0,\n",
              "  'template': ' Given hypothesis is this the right prediction? '},\n",
              " 4: {'premise': \"A man is due in court later charged with the murder 26 years ago of a teenager whose case was the first to be featured on BBC One's Crimewatch. Colette Aram, 16, was walking to her boyfriend's house in Keyworth, Nottinghamshire, on 30 October 1983 when she disappeared. Her body was later found in a field close to her home. Paul Stewart Hutchinson, 50, has been charged with murder and is due before Nottingham magistrates later.\",\n",
              "  'hypothesis': 'Paul Stewart Hutchinson is accused of having stabbed a girl.',\n",
              "  'idx': 4,\n",
              "  'label': 1,\n",
              "  'template': ' Given hypothesis is this the right prediction? '}}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"\"\" Decide whether the relation between premise and hypothesis have entailment or not entailment\n",
        "premise: No Weapons of Mass Destruction Found in Iraq Yet.\n",
        "hypothesis: Weapons of Mass Destruction Found in Iraq.\n",
        "relation: not entailment\"\"\""
      ],
      "metadata": {
        "id": "bz_dkpGQjdaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_3_prompts = []\n",
        "\n",
        "for i in range(0,5):\n",
        "  gpt_3_prompts.append(example + \"premise:\" + \n",
        "                       new_toy_set[i]['premise']  + '\\n' + 'hypothsis:' + \n",
        "                       new_toy_set[i]['hypothesis'] + \"\\n\" + \"relation: \")"
      ],
      "metadata": {
        "id": "WcqOgQ13erGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_3_prompts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG_nnu00e6MM",
        "outputId": "691f6f5d-1dc5-4619-f3fa-ed052c2e36b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Decide whether the relation between premise and hypothesis have entailment or not entailment\\npremise: No Weapons of Mass Destruction Found in Iraq Yet.\\nhypothesis: Weapons of Mass Destruction Found in Iraq.\\nrelation: not entailmentpremise:No Weapons of Mass Destruction Found in Iraq Yet.\\nhypothsis:Weapons of Mass Destruction Found in Iraq.\\nrelation: ',\n",
              " ' Decide whether the relation between premise and hypothesis have entailment or not entailment\\npremise: No Weapons of Mass Destruction Found in Iraq Yet.\\nhypothesis: Weapons of Mass Destruction Found in Iraq.\\nrelation: not entailmentpremise:A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.\\nhypothsis:Pope Benedict XVI is the new leader of the Roman Catholic Church.\\nrelation: ',\n",
              " ' Decide whether the relation between premise and hypothesis have entailment or not entailment\\npremise: No Weapons of Mass Destruction Found in Iraq Yet.\\nhypothesis: Weapons of Mass Destruction Found in Iraq.\\nrelation: not entailmentpremise:Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients.\\nhypothsis:Herceptin can be used to treat breast cancer.\\nrelation: ',\n",
              " ' Decide whether the relation between premise and hypothesis have entailment or not entailment\\npremise: No Weapons of Mass Destruction Found in Iraq Yet.\\nhypothesis: Weapons of Mass Destruction Found in Iraq.\\nrelation: not entailmentpremise:Judie Vivian, chief executive at ProMedica, a medical service company that helps sustain the 2-year-old Vietnam Heart Institute in Ho Chi Minh City (formerly Saigon), said that so far about 1,500 children have received treatment.\\nhypothsis:The previous name of Ho Chi Minh City was Saigon.\\nrelation: ',\n",
              " \" Decide whether the relation between premise and hypothesis have entailment or not entailment\\npremise: No Weapons of Mass Destruction Found in Iraq Yet.\\nhypothesis: Weapons of Mass Destruction Found in Iraq.\\nrelation: not entailmentpremise:A man is due in court later charged with the murder 26 years ago of a teenager whose case was the first to be featured on BBC One's Crimewatch. Colette Aram, 16, was walking to her boyfriend's house in Keyworth, Nottinghamshire, on 30 October 1983 when she disappeared. Her body was later found in a field close to her home. Paul Stewart Hutchinson, 50, has been charged with murder and is due before Nottingham magistrates later.\\nhypothsis:Paul Stewart Hutchinson is accused of having stabbed a girl.\\nrelation: \"]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_results = []\n",
        "for prompt in gpt_3_prompts:\n",
        "  # prompt gpt-3 with the input and get the output\n",
        "  response = openai.Completion.create(engine='text-davinci-001', prompt=prompt, max_tokens = 5, temperature=0)\n",
        "  # extract the text from the rest of the data\n",
        "  gpt_results.append(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "oVPeRqSqfHL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_text = [result.strip('\\n') for result in gpt_results]\n",
        "results_num = [0 if result == 'entailment' else 1 for result in results_text]"
      ],
      "metadata": {
        "id": "7_wyo4fDheym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT1xMXk3D5dT",
        "outputId": "3b3117d2-7399-485c-f74e-d967dbfd7b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['entailment', 'entailment', 'entailment', 'entailment', 'entailment']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXCAm-CoEJkB",
        "outputId": "b26b55f5-2782-4d07-c6f1-382c40d960f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# actual results from rte dataset\n",
        "true_label = [new_toy_set[sample]['label'] for sample in new_toy_set]\n",
        "print(true_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHHXxq7khgmD",
        "outputId": "9712bb68-a02a-43f5-dca7-2f7a4e3b677e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 0, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s8jM0BqcE3tu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}